
反反爬虫 
  一般流量比较大的网站都有反爬虫的机制，避免恶意的访问，减轻服务器的压力！

  策略： 设置抓取时间间隔
	更换代理
	模拟浏览器请求头headers
	加cookie
	用selenium模拟人的行为
	ajax加载的通过fiddler抓取请求分析 之后根据ajax请求 构造API请求的URL就可以直接获取想要的内容了，通常是json格式，反而不用去解析html了




1. scrapy架构 

?	Scrapy Engine(引擎): 负责Spider、6mPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。

?	
	Scheduler(调度器): 它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队（Queue），当引擎需要时，交还给引擎。


?	Downloader（下载器）：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理
，

?	Spider（爬虫）：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器)，


?	Item Pipeline(管道)：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方.


?	Downloader Middlewares（下载中间件）：你可以当作是一个可以自定义扩展下载功能的组件。


?	Spider Middlewares（Spider中间件）：你可以理解为是一个可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests）



2. 原理 运作流程
?	引擎：Hi！Spider, 你要处理哪一个网站？


?	Spider：老大要我处理xxxx.com。


?	引擎：你把第一个需要处理的URL给我吧。


?	Spider：给你，第一个URL是xxxxxxx.com。


?	引擎：Hi！调度器，我这有request请求你帮我排序入队一下。

?	调度器：好的，正在处理你等一下。


?	引擎：Hi！调度器，把你处理好的request请求给我。


?	调度器：给你，这是我处理好的request


?	引擎：Hi！下载器，你按照老大的下载中间件的设置帮我下载一下这个request请求


?	下载器：好的！给你，这是下载好的东西。（如果失败：sorry，这个request下载失败了。然后引擎告诉调度器，这个request下载失败了，你记录一下，我们待会儿再下载）

?	引擎：Hi！Spider，这是下载好的东西，并且已经按照老大的下载中间件处理过了，你自己处理一下（注意！这儿responses默认是交给def parse()这个函数处理的）


?	Spider：（处理完毕数据之后对于需要跟进的URL），
	Hi！引擎，我这里有两个结果，这个是我需要跟进的URL，还有这个是我获取到的Item数据。
?	引擎：Hi ！管道?我这儿有个item你帮我处理一下！调度器！这是需要跟进URL你帮我处理下。然后从第四步开始循环，直到获取完老大需要全部信息。


?	管道``调度器：好的，现在就做！

注意！只有当调度器中不存在任何request了，整个程序才会停止，（也就是说，对于下载失败的URL，Scrapy也会重新下载。）





scrapy-redis 分布式原理  

	scrapy-redis  基于redis的分布式爬虫框架 ，配合redis使用  sql和nosql的对比文章。

介绍一下分布式原理:
	前奏：
	scrapy-redis实现分布式，其实从原理上来说很简单，这里为描述方便，我们把自己的核心服务器称为master，而把用于跑爬虫程序的机器称为slave。
	scrapy框架抓取网页  需要首先给定它一些start_urls 首先访问start_urls里面的url
	步骤:
	我们在master上搭建一个redis数据库(注意这个数据库只作url的存储，不关心爬取的具体数据，不要和后面的mongodb和mysql混淆),
	并对每一个爬取网站类型，都开辟一个单独的列表字段  之后再slave里面来链接mast
	这样的结果就是，尽管有多个slave，然而大家获取url的地方只有一个，那就是服务器master上的redis数据库。
	由于scrapy-redis自身的队列机制，slave获取的链接不会相互冲突。这样各个slave在完成抓取任务之后，再把获取的结果汇总到服务器上（这时的数据存储不再在是redis，而是mongodb或者 mysql等存放具体内容的数据库了）



分布式的实现  ：
	1.使用两台机器，一台是win10，一台是centos7（详情请看http://www.111cn.net/sys/CentOS/63645.htm），分别在两台机器上部署scrapy来进行分布式抓取一个网站
	2..centos7的ip地址为192.168.1.112，用来作为redis的master端，win10的机器作为slave
	3..master的爬虫运行时会把提取到的url封装成request放到redis中的数据库 “dmoz:requests
	  并且从该数据库中提取request后下载网页，再把网页的内容存放到redis的另一个数据库中“dmoz:items”
	4.并且从该数据库中提取request后下载网页，再把网页的内容存放到redis的另一个数据库中“dmoz:items”
	5.重复上面的3和4，直到master的redis中的“dmoz:requests”数据库为空，再把master的redis中的“dmoz:items”数据库写入到mongodb中
	6.master里的reids还有一个数据“dmoz:dupefilter”是用来存储抓取过的url的指纹（使用哈希函数将url运算后的结果），是防止重复抓取的

 

	