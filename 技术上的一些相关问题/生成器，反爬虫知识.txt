生成器： 生成器是用来创建一个python序列的一个对象。使用它可以迭代庞大序列，且不需要在内存创建和存储整个序列。

生成器可以节省内存   函数中包含yield  生成器是迭代器 你只能遍历它一次  因为生成器没有将所有的值放入内存中，而是实时的生成这些值

它是一种实现迭代器的方式。除了yield表达式外，它和正常的函数没什么区别  
 
生成器（generator）
生成器，即生成一个容器。
在Python中，一边循环，一边计算的机制，称为生成器。
生成器可以理解为一种数据类型，这种数据类型自动实现了迭代器协议（其他数据类型需要调用自己的内置iter（）方法或__iter__()的内置函数），所以，生成器就是一个可迭代对象。
在Python中，使用生成器可以很方便的支持迭代器协议。
生成器优点
python使用生成器对延迟操作提供了支持，所谓延迟操作，是指在需要的时候才产生结果，而不是立即产生结果。

迭代器：对于序列类型：字符串、列表、元组，我们可以使用索引的方式迭代取出其包含的元素。但对于字典、集合、文件等类型是没有索引的，若还想取出其内部包含的元素，则必须找出一种不依赖于索引的迭代方式，这就是迭代器
16）Python的迭代器是什么？



Python中的迭代器是用来迭代包含一组元素的容器的，如列表。


9）参数是如何通过值或者引用传递的？

在Python中，一切都是对象，所有变量都是对象的引用。通过引用传递还是值传递要根据函数来确定（原文有语法错误，根据个人理解翻译）；事实上，你不能改变引用的值，但是如果对象是可变的，则可以改变对象。



常见的反爬虫：
	这几天在爬一个网站，网站做了很多反爬虫工作，爬起来有些艰难，花了一些时间才绕过反爬虫。在这里把我写爬虫以来遇到的各种反爬虫策略和应对的方法总结一下。

从功能上来讲，爬虫一般分为数据采集，处理，储存三个部分。这里我们只讨论数据采集部分。

一般网站从三个方面反爬虫：用户请求的Headers，用户行为，网站目录和数据加载方式。前两种比较容易遇到，大多数网站都从这些角度来反爬虫。第三种一些应用ajax的网站会采用，这样增大了爬取的难度。


通过Headers反爬虫：
	从用户请求的Headers反爬虫是最常见的反爬虫策略。很多网站都会对Headers的User-Agent进行检测，还有一部分网站会对Referer进行检测（一些资源网站的防盗链就是检测Referer）。如果遇到了这类反爬虫机制，
	可以直接在爬虫中添加Headers，将浏览器的User-Agent复制到爬虫的Headers中；或者将Referer值修改为目标网站域名。对于检测Headers的反爬虫，在爬虫中修改或者添加Headers就能很好的绕过。



基于用户行为反爬虫：
还有一部分网站是通过检测用户行为，例如同一IP短时间内多次访问同一页面，或者同一账户短时间内多次进行相同操作。

大多数网站都是前一种情况，对于这种情况，使用IP代理就可以解决。可以专门写一个爬虫，爬取网上公开的代理ip，检测后全部保存起来。这样的代理ip爬虫经常会用到，最好自己准备一个。有了大量代理ip后可以每请求几次更换一个ip，这在requests或者urllib2中很容易做到，这样就能很容易的绕过第一种反爬虫。

对于第二种情况，可以在每次请求后随机间隔几秒再进行下一次请求。有些有逻辑漏洞的网站，可以通过请求几次，退出登录，重新登录，继续请求来绕过同一账号短时间内不能多次进行相同请求的限制。



动态页面的反爬虫
上述的几种情况大多都是出现在静态页面，还有一部分网站，我们需要爬取的数据是通过ajax请求得到，或者通过JavaScript生成的。首先用Firebug或者HttpFox对网络请求进行分析。如果能够找到ajax请求，也能分析出具体的参数和响应的具体含义，我们就能采用上面的方法，直接利用requests或者urllib2模拟ajax请求，对响应的json进行分析得到需要的数据。


能够直接模拟ajax请求获取数据固然是极好的，但是有些网站把ajax请求的所有参数全部加密了。我们根本没办法构造自己所需要的数据的请求。我这几天爬的那个网站就是这样，除了加密ajax参数，它还把一些基本的功能都封装了，全部都是在调用自己的接口，而接口参数都是加密的。遇到这样的网站，我们就不能用上面的方法了，我用的是selenium+phantomJS框架，调用浏览器内核，并利用phantomJS执行js来模拟人为操作以及触发页面中的js脚本。从填写表单到点击按钮再到滚动页面，全部都可以模拟，不考虑具体的请求和响应过程，只是完完整整的把人浏览页面获取数据的过程模拟一遍。

用这套框架几乎能绕过大多数的反爬虫，因为它不是在伪装成浏览器来获取数据（上述的通过添加 Headers一定程度上就是为了伪装成浏览器），它本身就是浏览器，phantomJS就是一个没有界面的浏览器，只是操控这个浏览器的不是人。利用 selenium+phantomJS能干很多事情，例如识别点触式（12306）或者滑动式的验证码，对页面表单进行暴力破解等等。它在自动化渗透中还 会大展身手，以后还会提到这个。
